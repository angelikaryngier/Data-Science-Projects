{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "spam_data = pd.read_csv(\"D:\\Python\\data\\spam.csv\")\n",
    "\n",
    "spam_data['target'] = np.where(spam_data['target'] == 'spam', 1, 0)\n",
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of the documents in spam_data that are spam:  13.406317300789663\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test =train_test_split(spam_data['text'], spam_data['target'], random_state=0)\n",
    "\n",
    "print('Percentage of the documents in spam_data that are spam: ', spam_data['target'].mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest token in the vocabulary: com1win150ppmx3age16subscription\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer().fit(X_train)\n",
    "\n",
    "print('The longest token in the vocabulary:', sorted(count_vect.get_feature_names_out(), key=len, reverse=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9720812182741116\n"
     ]
    }
   ],
   "source": [
    "# transform the documents in the training data to a document-term matrix\n",
    "X_train_vectorized = count_vect.transform(X_train)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB(alpha=0.1).fit(X_train_vectorized, y_train)\n",
    "predictions = model.predict(count_vect.transform(X_test))\n",
    "#y_proba1 = model.predict_proba(count_vect.transform(X_test))[:,1]\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x7354 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 55130 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer().fit(X_train)\n",
    "\n",
    "X_train_vectorized_tfidf = tfidf_vect.transform(X_train)\n",
    "X_train_vectorized_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23731416, 0.36138463, 0.22862438, ..., 0.27814246, 0.36296157,\n",
       "       0.3487995 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the max tf-idf value for every feature\n",
    "feature_max_tfidf = X_train_vectorized_tfidf.max(0).toarray()[0]\n",
    "feature_max_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 features with the smallest max tf-idf:\n",
      "aaniye          0.074475\n",
      "athletic        0.074475\n",
      "chef            0.074475\n",
      "companion       0.074475\n",
      "courageous      0.074475\n",
      "dependable      0.074475\n",
      "determined      0.074475\n",
      "exterminator    0.074475\n",
      "healer          0.074475\n",
      "listener        0.074475\n",
      "organizer       0.074475\n",
      "pest            0.074475\n",
      "psychiatrist    0.074475\n",
      "psychologist    0.074475\n",
      "pudunga         0.074475\n",
      "stylist         0.074475\n",
      "sympathetic     0.074475\n",
      "venaam          0.074475\n",
      "afternoons      0.091250\n",
      "approaching     0.091250\n",
      "dtype: float64\n",
      "\n",
      "20 features with the largest max tf-idf:\n",
      "146tf150p    1.000000\n",
      "645          1.000000\n",
      "anything     1.000000\n",
      "anytime      1.000000\n",
      "beerage      1.000000\n",
      "done         1.000000\n",
      "er           1.000000\n",
      "havent       1.000000\n",
      "home         1.000000\n",
      "lei          1.000000\n",
      "nite         1.000000\n",
      "ok           1.000000\n",
      "okie         1.000000\n",
      "thank        1.000000\n",
      "thanx        1.000000\n",
      "too          1.000000\n",
      "where        1.000000\n",
      "yup          1.000000\n",
      "tick         0.980166\n",
      "blank        0.932702\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf_vect.get_feature_names_out()\n",
    "\n",
    "max_tfdif = pd.Series(feature_max_tfidf, index=feature_names)\n",
    "\n",
    "print('20 features with the smallest max tf-idf:\\n{}\\n'.format(max_tfdif.sort_index().sort_values(kind='stable')[:20]))\n",
    "\n",
    "print('20 features with the largest max tf-idf:\\n{}\\n'.format(max_tfdif.sort_index(ascending=False).sort_values(kind='stable')[:-21:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9954968337775665\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect2 = TfidfVectorizer(min_df=3).fit(X_train)\n",
    "\n",
    "X_train_vectorized_tfidf2 = tfidf_vect2.transform(X_train)\n",
    "\n",
    "model = MultinomialNB(alpha=0.1).fit(X_train_vectorized_tfidf2, y_train)\n",
    "y_proba = model.predict_proba(tfidf_vect2.transform(X_test))[:,1]\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, y_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of documents (number of characters) for not spam documents is 71.02 and for spam documents is 138.87.\n"
     ]
    }
   ],
   "source": [
    "avg_len_not_spam = spam_data['text'][spam_data['target']==0].aggregate(len).mean()\n",
    "\n",
    "avg_len_spam = spam_data['text'][spam_data['target']==1].aggregate(len).mean()\n",
    "\n",
    "print('Average length of documents (number of characters) for not spam documents is {:.2f} and for spam documents is {:.2f}.'\n",
    "      .format(avg_len_not_spam, avg_len_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to combine new features into the training data\n",
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9963202213809143\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect3 = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "X_train_vectorized_tfidf3 = tfidf_vect3.transform(X_train)\n",
    "\n",
    "X_train_vectorized_tfidf3 = add_feature(X_train_vectorized_tfidf3, X_train.aggregate(len))\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(C=10000).fit(X_train_vectorized_tfidf3, y_train)\n",
    "X_test_vectorized = tfidf_vect3.transform(X_test)\n",
    "X_test_vectorized = add_feature(X_test_vectorized, X_test.aggregate(len))\n",
    "y_scores = model.decision_function(X_test_vectorized)\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of digits per document for not spam documents is: 0.30 and for spam documents is: 15.76\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "mean_digits_not_spam = spam_data['text'][spam_data['target']==0].aggregate(lambda x: len(re.findall('\\d',x))).mean()\n",
    "mean_digits_spam = spam_data['text'][spam_data['target']==1].aggregate(lambda x: len(re.findall('\\d',x))).mean()\n",
    "\n",
    "print('The average number of digits per document for not spam documents is: {:.2f} and for spam documents is: {:.2f}'\n",
    "      .format(mean_digits_not_spam, mean_digits_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9972964025601413\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect4 = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train)\n",
    "X_train_vectorized_tfidf4 = tfidf_vect4.transform(X_train)\n",
    "\n",
    "X_train_number_of_digits = X_train.aggregate(lambda x: len(re.findall('\\d', x)))\n",
    "X_train_number_of_digits.name = 'digit_count'\n",
    "X_train_doc_len = X_train.aggregate(len)\n",
    "X_train_doc_len.name = 'length_of_doc'\n",
    "\n",
    "X_train_vectorized_tfidf4 = add_feature(X_train_vectorized_tfidf4, [X_train_number_of_digits, X_train_doc_len])\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(C=100, max_iter=1000).fit(X_train_vectorized_tfidf4, y_train)\n",
    "\n",
    "X_test_vectorized_tfidf4 = tfidf_vect4.transform(X_test)\n",
    "X_test_number_of_digits = X_test.aggregate(lambda x: len(re.findall('\\d', x)))\n",
    "X_test_doc_len = X_test.aggregate(len)\n",
    "X_test_vectorized_tfidf4 = add_feature(X_test_vectorized_tfidf4, [X_test_number_of_digits, X_test_doc_len])\n",
    "y_proba4 = model.predict_proba(X_test_vectorized_tfidf4)[:,1]\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, y_proba4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of non-word characters (anything other than a letter, digit or underscore) per document for not spam documents is: 17.29 and for spam documents is: 29.04 \n"
     ]
    }
   ],
   "source": [
    "avg_non_words_not_spam = spam_data['text'][spam_data['target']==0].agg(lambda x: len(re.findall('\\W', x))).mean()\n",
    "avg_non_words_spam = spam_data['text'][spam_data['target']==1].agg(lambda x: len(re.findall('\\W', x))).mean()\n",
    "\n",
    "print('The average number of non-word characters (anything other than a letter, digit or underscore) per document for not spam documents is: {:.2f} and for spam documents is: {:.2f} '\n",
    "      .format(avg_non_words_not_spam, avg_non_words_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9975637913179294\n"
     ]
    }
   ],
   "source": [
    "count_vect2 = CountVectorizer(min_df=5, ngram_range=(2,5), analyzer='char_wb').fit(X_train.iloc[:2000])\n",
    "X_train_count_vectorized2 = count_vect2.transform(X_train.iloc[:2000])\n",
    "\n",
    "X_train_non_words = X_train.agg(lambda x: len(re.findall('\\W',x)))\n",
    "X_train_non_words.name = 'non_word_char_count'\n",
    "\n",
    "X_train_count_vectorized2 = add_feature(X_train_count_vectorized2, [X_train_doc_len.iloc[:2000],\n",
    "                                                                    X_train_number_of_digits.iloc[:2000],\n",
    "                                                                    X_train_non_words.iloc[:2000]])\n",
    "\n",
    "model = LogisticRegression(C=100,max_iter=1000).fit(X_train_count_vectorized2, y_train.iloc[:2000])\n",
    "\n",
    "X_test_count_vectorized2 = count_vect2.transform(X_test)\n",
    "X_test_non_words = X_test.agg(lambda x: len(re.findall('\\W', x)))\n",
    "X_test_count_vectorized2 = add_feature(X_test_count_vectorized2, [X_test_doc_len,\n",
    "                                                                  X_test_number_of_digits,\n",
    "                                                                  X_test_non_words])\n",
    "y_proba_count2 = model.predict_proba(X_test_count_vectorized2)[:,1]\n",
    "print('AUC: ', roc_auc_score(y_test, y_proba_count2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 smallest coefficients from the model:\n",
      "['n ', ' i', 'at', 'he', ' m', '..', 'us', 'go', ' lo', ' bu']\n",
      "\n",
      "10 largest coefficients from the model:\n",
      "['digit_count', 'ne', ' st', 'co', 's ', 'xt', 'lt', 'xt ', ' ne', 'der']\n"
     ]
    }
   ],
   "source": [
    "feature_names_count2 = count_vect2.get_feature_names_out()\n",
    "feature_names_count2 = np.append(feature_names_count2, ['length_of_doc', 'digit_count', 'non_word_char_count'])\n",
    "feature_names_count2\n",
    "\n",
    "coef_count_2 = pd.Series(model.coef_[0], index=feature_names_count2)\n",
    "\n",
    "print('10 smallest coefficients from the model:\\n{}\\n'\n",
    "      .format(coef_count_2.sort_index().sort_values(kind='stable').index[:10].tolist()))\n",
    "\n",
    "print('10 largest coefficients from the model:\\n{}'\n",
    "      .format(coef_count_2.sort_index(ascending=False).sort_values(kind='stable').index[:-11:-1].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
